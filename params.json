{"name":"Multithreaded crawler","tagline":"A condensed crawler framework of multithreaded model","body":"multithreaded_crawler\r\n=====================\r\n\r\nA condensed crawler framework of “multithreaded model”\r\n\r\n\r\ndependency\r\n=====================\r\nAt present, the framework depends on nothing except for modules in the python standard libraries.\r\n\r\n\r\nUsage\r\n=====================\r\n`cd threaded_spider`  \r\n`python run.py --help`  \r\n\r\nYou will see a demo output by `python run.py`, it crawls the sina.com.cn using five threads \r\nand has the crawling depth limited to be 2 by default (It's tested in python2.7).    \r\nIn threaded_spider directory, there are extra log files whose name like “spider.*.log” respectively \r\ngenerated using `python run.py --thread=*` command.  \r\n\r\n\r\nCommunity\r\n======================\r\nQQ Group: 4704309  \r\nYour contribute will be welcome.  \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}